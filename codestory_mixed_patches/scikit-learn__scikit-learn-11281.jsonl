{
    "instance_id": "scikit-learn__scikit-learn-11281",
    "model_name_or_path": "codestory-mixed",
    "model_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a9f66740f..2829dea79 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -71,9 +71,10 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n     provides basic common methods for mixture models.\n     \"\"\"\n \n-    def __init__(self, n_components, tol, reg_covar,\n-                 max_iter, n_init, init_params, random_state, warm_start,\n-                 verbose, verbose_interval):\n+    def __init__(self, n_clusters=None, n_components=None, tol=1e-3, reg_covar=1e-6,\n+                 max_iter=100, n_init=1, init_params='kmeans', random_state=None,\n+                 warm_start=False, verbose=0, verbose_interval=10):\n+        self.n_clusters = n_clusters\n         self.n_components = n_components\n         self.tol = tol\n         self.reg_covar = reg_covar\n@@ -85,6 +86,14 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n         self.verbose = verbose\n         self.verbose_interval = verbose_interval\n \n+        # Handle deprecation of n_components\n+        if n_components is not None:\n+            warnings.warn(\"The 'n_components' parameter is deprecated and will be removed in a future version. \"\n+                          \"Use 'n_clusters' instead.\", DeprecationWarning)\n+            self.n_clusters = n_components\n+        elif n_clusters is None:\n+            raise ValueError(\"Either 'n_clusters' or 'n_components' must be specified.\")\n+\n     def _check_initial_parameters(self, X):\n         \"\"\"Check values of the basic parameters.\n \n@@ -240,6 +249,10 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n \n+        # Compute and store the cluster assignments\n+        _, log_resp = self._estimate_log_prob_resp(X)\n+        self.labels_ = log_resp.argmax(axis=1)\n+\n         return self\n \n     def _e_step(self, X):\n",
    "resolved": false
}