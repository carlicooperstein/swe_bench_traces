{
    "instance_id": "pytest-dev__pytest-7490",
    "model_name_or_path": "codestory-mixed",
    "model_patch": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..9f03edc2b 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -233,13 +233,18 @@ unexpectedsuccess_key = StoreKey[str]()\n def pytest_runtest_setup(item: Item) -> None:\n     item._store[skipped_by_mark_key] = False\n \n+    # Evaluate skip and skipif marks\n     skipped = evaluate_skip_marks(item)\n     if skipped:\n         item._store[skipped_by_mark_key] = True\n         skip(skipped.reason)\n \n+    # Evaluate xfail marks\n     if not item.config.option.runxfail:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n+\n+        # If there's an xfail marker and run is False, mark the test as xfail\n         if xfailed and not xfailed.run:\n             xfail(\"[NOTRUN] \" + xfailed.reason)\n \n@@ -248,7 +253,8 @@ def pytest_runtest_setup(item: Item) -> None:\n def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n     xfailed = item._store.get(xfailed_key, None)\n     if xfailed is None:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+        xfailed = evaluate_xfail_marks(item)\n+        item._store[xfailed_key] = xfailed\n \n     if not item.config.option.runxfail:\n         if xfailed and not xfailed.run:\n",
    "resolved": false
}