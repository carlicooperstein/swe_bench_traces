{
    "instance_id": "scikit-learn__scikit-learn-13241",
    "model_name_or_path": "codestory-mixed",
    "model_patch": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex bb91d0cbb..932a5916c 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -178,6 +178,14 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n                                 filter_params=True, n_jobs=self.n_jobs,\n                                 **params)\n \n+    def _normalize_eigenvectors(self, eigenvectors):\n+        \"\"\"Normalize eigenvectors to ensure consistent signs.\"\"\"\n+        for i in range(eigenvectors.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(eigenvectors[:, i]))\n+            if eigenvectors[max_abs_idx, i] < 0:\n+                eigenvectors[:, i] *= -1\n+        return eigenvectors\n+\n     def _fit_transform(self, K):\n         \"\"\" Fit's using kernel K\"\"\"\n         # center kernel\n@@ -210,6 +218,9 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n                                                 maxiter=self.max_iter,\n                                                 v0=v0)\n \n+        # normalize eigenvectors to ensure consistent signs\n+        self.alphas_ = self._normalize_eigenvectors(self.alphas_)\n+\n         # sort eigenvectors in descending order\n         indices = self.lambdas_.argsort()[::-1]\n         self.lambdas_ = self.lambdas_[indices]\n@@ -321,3 +332,89 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         K = self._get_kernel(X, self.X_transformed_fit_)\n \n         return np.dot(K, self.dual_coef_)\n+    def _normalize_eigenvectors(self, eigenvectors):\n+        \"\"\"Normalize eigenvectors to ensure consistent signs.\"\"\"\n+        for i in range(eigenvectors.shape[1]):\n+            max_abs_idx = np.argmax(np.abs(eigenvectors[:, i]))\n+            if eigenvectors[max_abs_idx, i] < 0:\n+                eigenvectors[:, i] *= -1\n+        return eigenvectors\n+\n+    def _fit_transform(self, K):\n+        \"\"\" Fit's using kernel K\"\"\"\n+        # center kernel\n+        K = self._centerer.fit_transform(K)\n+\n+        if self.n_components is None:\n+            n_components = K.shape[0]\n+        else:\n+            n_components = min(K.shape[0], self.n_components)\n+\n+        # compute eigenvectors\n+        if self.eigen_solver == 'auto':\n+            if K.shape[0] > 200 and n_components < 10:\n+                eigen_solver = 'arpack'\n+            else:\n+                eigen_solver = 'dense'\n+        else:\n+            eigen_solver = self.eigen_solver\n+\n+        if eigen_solver == 'dense':\n+            self.lambdas_, self.alphas_ = linalg.eigh(\n+                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n+        elif eigen_solver == 'arpack':\n+            random_state = check_random_state(self.random_state)\n+            # initialize with [-1,1] as in ARPACK\n+            v0 = random_state.uniform(-1, 1, K.shape[0])\n+            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n+                                                which=\"LA\",\n+                                                tol=self.tol,\n+                                                maxiter=self.max_iter,\n+                                                v0=v0)\n+\n+        # sort eigenvectors in descending order\n+        indices = self.lambdas_.argsort()[::-1]\n+        self.lambdas_ = self.lambdas_[indices]\n+        self.alphas_ = self.alphas_[:, indices]\n+\n+        # normalize eigenvectors to ensure consistent signs\n+        self.alphas_ = self._normalize_eigenvectors(self.alphas_)\n+\n+        # remove eigenvectors with a zero eigenvalue\n+        if self.remove_zero_eig or self.n_components is None:\n+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n+\n+        return K\n+\n+    def fit(self, X, y=None):\n+        \"\"\"Fit the model from data in X.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            Training vector, where n_samples in the number of samples\n+            and n_features is the number of features.\n+\n+        Returns\n+        -------\n+        self : object\n+            Returns the instance itself.\n+\n+        Notes\n+        -----\n+        This method includes sign normalization of eigenvectors to ensure\n+        consistent results across different runs.\n+        \"\"\"\n+        X = check_array(X, accept_sparse='csr', copy=self.copy_X)\n+        self._centerer = KernelCenterer()\n+        K = self._get_kernel(X)\n+        self._fit_transform(K)\n+\n+        if self.fit_inverse_transform:\n+            sqrt_lambdas = np.diag(np.sqrt(self.lambdas_))\n+            X_transformed = np.dot(self.alphas_, sqrt_lambdas)\n+            self._fit_inverse_transform(X_transformed, X)\n+\n+        self.X_fit_ = X\n+        return self\n",
    "resolved": true
}